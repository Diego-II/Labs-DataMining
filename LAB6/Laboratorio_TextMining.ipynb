{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrantes\n",
    "\n",
    "1. \n",
    "2. \n",
    "3. \n",
    "4. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instrucciones\n",
    "\n",
    "El laboratorio tiene 6ptos, donde obtener 6 ptos equivale a un 7.0 y 0 ptos a un 1.0. El formato de entrega será subir a u-cursos un **único** jupyter notebook por grupo.\n",
    "El notebook debe ejecutar desde la primera celda a la última **sin errores**. Todo el código debe estar en el mismo notebook. El código debe estar en un formato **presentable** y **ejecutado**.\n",
    "\n",
    "\n",
    "**Nota:** Se descontará puntaje si adjunta gráficos sin descripción. Para esto puede añadir una descripción en los atributos xlabel, ylabel y título; o añadir una celda markdown debajo del gráfico de la forma \"**Figura x:** descripción detallada...\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problema\n",
    "\n",
    "Neural Information Processing Systems (NIPS) es una de las conferencias más importantes sobre machine learning en el mundo. El objetivo de este laboratorio es hallar los temás presentes en las publicaciones científicas (papers) presentados en esta conferencia, de esta manera una persona no muy entendida en el tema de machine learning puede tener una idea de las distintas líneas de investigación que existen. La base de datos con la que cuenta para repsonder esta pregunta incluye títulos, autores, abstract, y el texto presente de todos los papers del NIPS desde 1987 hasta el 2016. Por simplicidad, se trabaja solo con los abstract de los papers que son guardados en la variable **corpus**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>year</th>\n",
       "      <th>title</th>\n",
       "      <th>event_type</th>\n",
       "      <th>pdf_name</th>\n",
       "      <th>abstract</th>\n",
       "      <th>paper_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1987</td>\n",
       "      <td>Self-Organization of Associative Database and ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1-self-organization-of-associative-database-an...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>1987</td>\n",
       "      <td>A Mean Field Theory of Layer IV of Visual Cort...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10-a-mean-field-theory-of-layer-iv-of-visual-c...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>683\\n\\nA MEAN FIELD THEORY OF LAYER IV OF VISU...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100</td>\n",
       "      <td>1988</td>\n",
       "      <td>Storing Covariance by the Associative Long-Ter...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100-storing-covariance-by-the-associative-long...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>394\\n\\nSTORING COVARIANCE BY THE ASSOCIATIVE\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000</td>\n",
       "      <td>1994</td>\n",
       "      <td>Bayesian Query Construction for Neural Network...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1000-bayesian-query-construction-for-neural-ne...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>Bayesian Query Construction for Neural\\nNetwor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1001</td>\n",
       "      <td>1994</td>\n",
       "      <td>Neural Network Ensembles, Cross Validation, an...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1001-neural-network-ensembles-cross-validation...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>Neural Network Ensembles, Cross\\nValidation, a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id  year                                              title event_type  \\\n",
       "0     1  1987  Self-Organization of Associative Database and ...        NaN   \n",
       "1    10  1987  A Mean Field Theory of Layer IV of Visual Cort...        NaN   \n",
       "2   100  1988  Storing Covariance by the Associative Long-Ter...        NaN   \n",
       "3  1000  1994  Bayesian Query Construction for Neural Network...        NaN   \n",
       "4  1001  1994  Neural Network Ensembles, Cross Validation, an...        NaN   \n",
       "\n",
       "                                            pdf_name          abstract  \\\n",
       "0  1-self-organization-of-associative-database-an...  Abstract Missing   \n",
       "1  10-a-mean-field-theory-of-layer-iv-of-visual-c...  Abstract Missing   \n",
       "2  100-storing-covariance-by-the-associative-long...  Abstract Missing   \n",
       "3  1000-bayesian-query-construction-for-neural-ne...  Abstract Missing   \n",
       "4  1001-neural-network-ensembles-cross-validation...  Abstract Missing   \n",
       "\n",
       "                                          paper_text  \n",
       "0  767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABA...  \n",
       "1  683\\n\\nA MEAN FIELD THEORY OF LAYER IV OF VISU...  \n",
       "2  394\\n\\nSTORING COVARIANCE BY THE ASSOCIATIVE\\n...  \n",
       "3  Bayesian Query Construction for Neural\\nNetwor...  \n",
       "4  Neural Network Ensembles, Cross\\nValidation, a...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/papers.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Non-negative matrix factorization (NMF) has previously been shown to \\r\\nbe a useful decomposition for multivariate data. Two different multi- \\r\\nplicative algorithms for NMF are analyzed. They differ only slightly in \\r\\nthe multiplicative factor used in the update rules. One algorithm can be \\r\\nshown to minimize the conventional least squares error while the other \\r\\nminimizes the generalized Kullback-Leibler divergence. The monotonic \\r\\nconvergence of both algorithms can be proven using an auxiliary func- \\r\\ntion analogous to that used for proving convergence of the Expectation- \\r\\nMaximization algorithm. The algorithms can also be interpreted as diag- \\r\\nonally rescaled gradient descent, where the rescaling factor is optimally \\r\\nchosen to ensure convergence. '"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = df[df[\"abstract\"]!='Abstract Missing'][\"abstract\"].tolist()\n",
    "corpus[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A. Investigación (1.5 ptos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A.1 ¿Que es el modelado de tópicos? (0.75 ptos)\n",
    "\n",
    "Explique en que consiste el modelado de tópicos, para qué sirve y de dos ejemplos de uso distintos al problema del laboratorio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A.2 ¿Qué es LDA? (0.75 pto)\n",
    "\n",
    "Explique en palabras qué es LDA, qué es $K$, $\\phi_{k}$ y $\\pi_{d}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B. Preparación (2.5 ptos)\n",
    "\n",
    "Esta sección tiene por objetivo crear una base de datos consolidada para el uso de modelos de tópicos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B.1 Análisis exploratorio inicial (0.25 ptos)\n",
    "\n",
    "Compute la cantidad de tokens y el tamaño del vocabulario del corpus sin realizar ningún tipo de procesamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B.2 Limpieza de los datos (1.0 pto)\n",
    "\n",
    "Cree una función de tokenización que procese los papers. Escoja un ejemplo del conjunto de datos y muestra como se ve antes y despues de aplicar el procesamiento. Esta función debe eliminar patrónes de caracteres característicos del corpus que no aportan información (por ejemplo: \"\\r\") y eliminar stopwords que se encuentran en el archivo adjunto (**stopwords.txt**). Compute la cantidad de tokens y el tamaño del vocabulario resultante de aplicar este nivel de procesamiento al corpus. ¿Por qué se deben eliminar las stopwords?\n",
    "\n",
    "**Hint**: para eliminar patrónes de caracteres puede usar la función **re.sub()**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B.3 Post procesamiento (1.0)\n",
    "\n",
    "Realice un postprocesamietno del corpus que sirva para reducir aún más el vocabulario: análisis de palabras muy frecuentes, poco frecuentes, errores ortográficos, etc. Compute la cantidad de tokens y el tamaño del vocabulario resultante de aplicar este nivel de procesamiento al corpus. Complete el cuadro resumen de estadísticas.\n",
    "\n",
    "|Etapa|tokens|vocabulario|\n",
    "|---|---|---|\n",
    "|A.1|||\n",
    "|A.2|||\n",
    "|A.3|||"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C. LDA (2ptos)\n",
    "\n",
    "En este sección tiene por objetivo descubrir los tópicos presentes en el corpus. Documentación de LDA:\n",
    "\n",
    "https://radimrehurek.com/gensim/models/ldamodel.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C.1 Corpus (0.5 ptos)\n",
    "\n",
    "Genere el corpus el formato que lda requiere e imprima un ejemplo de un documento en dicho formato y en el formato original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.ldamodel import LdaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C.2  Interpretación de los tópicos (1.5 ptos)\n",
    "Entrene LDA para que encuentre 10 tópicos ($K=10$). Luego, interprete los tópicos encontrados. Para esto se puede apollar del método **.show_topics**  o de la visualización de **pyLDAvis**. Responde a la siguiente pregunta: ¿Cuáles son los tópicos más relevantes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.ldamodel import LdaModel\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
